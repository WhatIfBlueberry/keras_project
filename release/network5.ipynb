{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a98f4c50-8767-4319-b7de-798d483e40ab",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This project is about creating a convolutional neural network to recognize images of the playing card game [fantastische reiche](https://www.spiel-des-jahres.de/spiele/fantastische-reiche/). If you have never heard of it - give it a try. Especially if you like deck-building games, this one will be a lot of fun. So why do i want to detect cards? Well, at the end of the game you have to count points based on the cards in your hand. This maybe does not sound too difficult, but the cards all influence each other! E.g. \"Flash\" counts much more while you also have \"Thunderstorm\". Your \"Elven Archers\" improve when you have *no* weather-type card and your mighty \"magic wand\" is obviously useless if you don't have at least a single wizard. You see things can get pretty complicated. Luckly the creators of the game provide you with an official [score calculator](https://fantastische-reiche.de/). This works great - but you have to select each card you have individualy from a list of all playing cards. Wouldn't it be awesome if we could make this process just a little bit simpler? \n",
    "\n",
    "## Aim\n",
    "I would like to write an app in the near future which allows to detect the cards in your hand and calculate the final score. I imagine it to work like a QR-Code scanner, where you just point at your cards one after another to add them to the calculations. No pushing of the \"shutter\"-button of the camera required. \n",
    "This project will be one part of the final goal. I want to train a small, easy to understand yet accurate model which can be used locally on a phone. Altough this was not part of the lecture, i also wrote a (very basic!) android application out of curiousity to see how i can use my model trained in keras in \"real-life\"-applications. (Hopefully you can share the excitement i felt when it worked for the first time and i was able to detect cards using my phone. So cool)\n",
    "\n",
    "## About the Code\n",
    "I do not like to have a lot of inline-comments. I will give you some insight in my reasoning and the development process between the cells. Enjoy!\n",
    "\n",
    "> “Simplicity is the final achievement. \\\n",
    "> After one has played a vast quantity of notes and more notes, it is simplicity that emerges as the crowning reward of art.” "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8a92c92-e421-403b-8d7c-746624898aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8405ed-8607-4c51-8c0c-00542455b311",
   "metadata": {},
   "source": [
    "As you can imagine optaining the data was a big part of the project, since no dataset is availbile. There are 53 different playing cards, and i wanted to have at least 50 (better 100) images for each card. I spend a couple of hours taking photos and optimizing the camera app on my smartphone so the processing wouldn't take as long and i can shoot photos faster. Have you ever had your phone crash because you took too many photos in succession? Well, i certanly haven't before this project. After my phone froze for the third time i sat on my bad at 2am and thought there had to be a better way to do this. And i had an idea! What if instead of taking photos, i would do videos of the cards, showing them from different angles? I ended up doing roughly 30 seconds video of each card, and then extracted the individual frames using the command line tool [ffmpeg](https://www.ffmpeg.org/). With this, i had ~25.000 images as training / validation data within half an hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab730db2-3a1b-4882-ab87-a8af4dc24d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPathLocal = \"G:/My Drive/keras_project/dataDigits\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67852ab-f238-4f4d-9226-5c886adb8e92",
   "metadata": {},
   "source": [
    "I tested multiple different resolutions for the image. Some square, some rectangle. I thought since the cards are not square a narrower image would highlight more important features of the card. To my surprise this was not really the case. This (pretty small) resolution of 64x64 pixels showed no worse performance than other resolutions like 128x128 or 320x180 which i tried - while being significantly faster to train. I can imagine that larger resolutions would be relevant if the photo was taken from far away or the cards would be more \"hidden\" in the image. For my application, this is not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2d896d2-7fbc-4d61-9812-670bd560cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_classes = 53\n",
    "\n",
    "img_height = 64\n",
    "img_width = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03107154-6f66-4c56-8915-d31d688a5151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25757 files belonging to 53 classes.\n",
      "Using 20606 files for training.\n",
      "Using 5151 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds = keras.utils.image_dataset_from_directory(\n",
    "    dataPathLocal,\n",
    "    validation_split=0.2,\n",
    "    subset=\"both\",\n",
    "    seed=42,\n",
    "    image_size=(img_height, img_width),\n",
    "     batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c56f00-63e3-4964-a314-f4a88ce8048e",
   "metadata": {},
   "source": [
    "Some caching that was recommended in the keras documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c9ad619-683d-428e-a028-b9cd9eaa782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd13e90-eb59-466d-8908-fc388c9d8fbb",
   "metadata": {},
   "source": [
    "This model is very simililar to the one in the [documentation](https://www.tensorflow.org/tutorials/images/classification) of image classification on tensorflow. I tried mutliple (much more complicated) models, most showed good performance. I ended up with this is one, as it is simple, small and performs well. There is honestly not much to explain: I used rotation and zoom as data augmentation. The pixel-values are scaled from integers to floating points [0,255] -> (0,1). Three convolution layers with an increasing amount of filters and a constant kernel size of 3 to extract features and a final dense layer with 128 neurons. I used 'relu' as activation function for each layer. I thought about using 'softmax' for the dense layer, and while it also showed good results, the training was significantly slower. 'relu' worked fine out of the box so thats what i ended up with. Softmax is used on the networks output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adf246e1-77bb-4820-904a-83d6cfe50e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=(img_height, img_width, 3)),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.Rescaling(1./255),\n",
    "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes, name=\"outputs\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ba9bc2-9f37-4dec-9ffe-7018450fb794",
   "metadata": {},
   "source": [
    "I used the compile options recommended by Keras for this kind of problem. <code>SparseCategoricalCrossentropy</code> expects the labels as integer which was easier for me to do (using the folder names) than dealing with one-hot encoded labels using <code>CategoricalCrossentropy</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54a426c0-c569-4874-8356-bd220cde64a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68a86ec1-64b4-4710-a047-542b60206c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ random_rotation                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomRotation</span>)                │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ random_zoom (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomZoom</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ outputs (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">53</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,837</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ random_rotation                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mRandomRotation\u001b[0m)                │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ random_zoom (\u001b[38;5;33mRandomZoom\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ rescaling (\u001b[38;5;33mRescaling\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │           \u001b[38;5;34m448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m4,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m524,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ outputs (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m53\u001b[0m)             │         \u001b[38;5;34m6,837\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">554,837</span> (2.12 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m554,837\u001b[0m (2.12 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">554,837</span> (2.12 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m554,837\u001b[0m (2.12 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373e1a26-bc26-4c34-bbeb-ed24698ca124",
   "metadata": {},
   "source": [
    "The network learned surprisingly fast. After three epochs (644 * 32 ~ 20k images each) the accuracy was already at around 80%! To be honest i expected this to be way more difficult, we have 53 different classes after all. I trained this network multiple times with an increasing amount of epochs. The last learning session was over the duration of 50 epochs and the highest validation accuracy i observed was 0.9996, or 99.96%. While this sounds amazing and i'm very happy with the result, i have to admit that the the training data is very similar. For each card the training data consists of one video, with my wooden desk as background and the same lighting conditions. I used this model in the android application i talked about in the introduction, and while it works great in most cases, its accuracy for real life use is not 99.96%. (maybe 85%). Taking the time to create more training data would certanly improve the (still awesome!) 85% effective accuracy even more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "feae2776-c685-45bc-85b4-c76af878597a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 60ms/step - accuracy: 0.1851 - loss: 3.1301 - val_accuracy: 0.7022 - val_loss: 0.9291\n",
      "Epoch 2/5\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - accuracy: 0.8396 - loss: 0.5435 - val_accuracy: 0.8944 - val_loss: 0.3256\n",
      "Epoch 3/5\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - accuracy: 0.9420 - loss: 0.1912 - val_accuracy: 0.8872 - val_loss: 0.3529\n",
      "Epoch 4/5\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - accuracy: 0.9651 - loss: 0.1170 - val_accuracy: 0.9740 - val_loss: 0.0884\n",
      "Epoch 5/5\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - accuracy: 0.9764 - loss: 0.0791 - val_accuracy: 0.8944 - val_loss: 0.3551\n"
     ]
    }
   ],
   "source": [
    "epochs=5\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8ce408-a4f1-4f95-b28b-819dff43bcce",
   "metadata": {},
   "source": [
    "I used <code>SparseCategoricalCrossentropy</code> as loss-function. This means the labels are integer values (23, 44, 51 etc.). Since i am interested in the name of the card and not the integer value which i can not validate easily i created an array with all card names, and sorted them alphabetically. The training data in the integer-labled folders is also sorted alphabetically. So if the network outputs <code>13</code> as prediction, i just have to look up <code>card_names_sep[13]</code> to get the name of the card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01b08d3d-ac65-4007-91fc-84856aaab22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "card_names = \"kaiserin,koenig,koenigin,kriegsherr,prinzessin,elbenschuetzen,leichte_kavallerie,ritter,waldlaeufer,zwergeninfanterie,buch_der_veraenderung,juwel_der_ordnung,rune_des_schutzes,schild_von_keth,weltenbaum,basilisk,drache,einhorn,hydra,schlachtross,blitz,buschfeuer,feuerwesen,kerze,schmiede,grosse_flut,insel,quelle_des_lebens,sumpf,wasserwesen,erdwesen,gebirge,glockenturm,hoehle,wald,elbischer_bogen,kampfzeppelin,kriegsschiff,schwert_von_keth,zauberstab,blizzard,luftwesen,rauch,regensturm,wirbelsturm,hexenmeister,sammler,herr_der_bestien,totenbeschwoerer,magierin,doppelgaenger,gestaltenwandler,spiegelung\"\n",
    "card_names_sep = card_names.split(\",\")\n",
    "card_names_sep.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2057d40-3e94-45ac-bcad-726df276ba71",
   "metadata": {},
   "source": [
    "This is just some manual validation i used before i build the app implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "559791e9-cd85-40ff-9b6c-b6009e86aee3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Test/Desktop/zauberstab.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/Test/Desktop/zauberstab.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m img \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mload_img(\n\u001b[0;32m      4\u001b[0m     path, target_size\u001b[38;5;241m=\u001b[39m(img_height, img_width)\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      6\u001b[0m img_array \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mimg_to_array(img)\n\u001b[0;32m      7\u001b[0m img_array \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(img_array, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\image_utils.py:235\u001b[0m, in \u001b[0;36mload_img\u001b[1;34m(path, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, pathlib\u001b[38;5;241m.\u001b[39mPath):\n\u001b[0;32m    234\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(path\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[1;32m--> 235\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    236\u001b[0m         img \u001b[38;5;241m=\u001b[39m pil_image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(f\u001b[38;5;241m.\u001b[39mread()))\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Test/Desktop/zauberstab.jpg'"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/Test/Desktop/zauberstab.jpg\"\n",
    "\n",
    "img = tf.keras.utils.load_img(\n",
    "    path, target_size=(img_height, img_width)\n",
    ")\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "pos = np.argmax(score)\n",
    "prediction = card_names_sep[pos]\n",
    "\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(prediction, 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254512e5-ffa4-433f-bf81-856ab6bd88df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"C:/Users/Test/Desktop/WiSe24/nn/project/model/model_small_50.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c6020-0610-4ebc-aa3e-023b6b975dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"C:/Users/Test/Desktop/WiSe24/nn/project/model/realms.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe57196-b2e7-4e8e-84d7-c83e398b7225",
   "metadata": {},
   "source": [
    "For use in Android Studio, i had to migrate the keras model to TFLite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f30a20-0aff-4cab-b818-afa6d7fbeb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"C:/Users/Test/Desktop/WiSe24/nn/project/model/model_small_50.keras\")\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('model_small_50.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df0b7cc-e866-460b-b076-12864eba0c9f",
   "metadata": {},
   "source": [
    ".. and this was just a visualization out of curiosity :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6348cfef-e87a-4166-adc8-5defabc20203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import ImageFont\n",
    "\n",
    "visualkeras.layered_view(model, legend=True)\n",
    "font = ImageFont.truetype(\"arial.ttf\", 12)\n",
    "visualkeras.layered_view(model, legend=True, font=font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f55bab-ee56-4edb-bb2f-3abe08d13d03",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
